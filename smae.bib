Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Zack2000,
abstract = {This is a response to the special issue of Organization Science on Jazz Improvisation and Organizing (Vol. 9, No. 5, 1998). It is a call to unpack the jazz metaphor by extending the notion of jazz, and thereby the value of the metaphor, beyond the limited definition described in the issue. In that issue, jazz was described as a process of improvising within a highly constrained structure and set of rules. Other genres of jazz, however, have gone beyond those constraints. Jazz improvisation has occurred within forms, with forms, and beyond forms. Perhaps organizational improvisation may as well.},
author = {Zack, Michael H.},
doi = {10.1287/orsc.11.2.227.12507},
issn = {1047-7039},
journal = {Organization Science},
number = {2},
pages = {227--234},
title = {{Jazz Improvisation and Organizing: Once More from the Top}},
url = {https://www.jstor.org/stable/2640286},
volume = {11},
year = {2000}
}
@incollection{Mantaras2017,
author = {de M{\'{a}}ntaras, Ram{\'{o}}n L{\'{o}}pez},
booktitle = {The Next Step: Exponential Life},
pages = {100--124},
title = {{Artificial Intelligence and the Arts: Toward Computational Creativity}},
url = {https://www.bbvaopenmind.com/en/articles/artificial-intelligence-and-the-arts-toward-computational-creativity/ https://www.bbvaopenmind.com/wp-content/uploads/2017/03/BBVA-OpenMind-book-The-Next-Step-Exponential-Life-1-1.pdf},
year = {2017}
}
@article{Vargas2019,
abstract = {Deep Reinforcement Learning (DRL) has emerged as a powerful control technique in robotic science. In contrast to control theory, DRL is more robust in the thorough exploration of the environment. This capability of DRL generates more human-like behaviour and intelligence when applied to the robots. To explore this capability, we designed challenging manipulation tasks to observe robots strategy to handle complex scenarios. We observed that robots not only perform tasks successfully, but also transpire a creative and non intuitive solution. We also observed robot's persistence in tasks that are close to success and its striking ability in discerning to continue or give up.},
archivePrefix = {arXiv},
arxivId = {1910.07459},
author = {Vargas, Juan Carlos and Bhoite, Malhar and Farimani, Amir Barati},
eprint = {1910.07459},
file = {:Users/marcoyel21/Library/Application Support/Mendeley Desktop/Downloaded/Vargas, Bhoite, Farimani - 2019 - Creativity in Robot Manipulation with Deep Reinforcement Learning.pdf:pdf},
title = {{Creativity in Robot Manipulation with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1910.07459},
year = {2019}
}
@misc{openia,
abstract = {We've observed agents discovering progressively more complex tool use while playing a simple game of hide-and-seek. Through training in our new simulated hide-and-seek environment, agents build a series of six distinct strategies and counterstrategies, some of which we did not know our environment supported. The self-supervised emergent complexity in this simple environment further suggests that multi-agent co-adaptation may one day produce extremely complex and intelligent behavior.},
author = {{Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew}, Igor Mordatch},
booktitle = {OpenAI},
title = {{Emergent Tool Use from Multi-Agent Interaction}},
url = {https://openai.com/blog/emergent-tool-use/},
urldate = {2021-08-27},
year = {2019}
}
@article{Pryor1969,
abstract = {Two rough-toothed porpoises (Steno bredanensis) were individually trained to emit novel responses, which were not developed by shaping and which were not previously known to occur in the species, by reinforcing a different response to the same set of stimuli in each of a series of training sessions. A technique was developed for transcribing a complex series of behaviors on to a single cumulative record so that the training sessions of the second animal could be fully recorded. Cumulative records are presented for a session in which the criterion that only novel behaviors would be reinforced was abruptly met with four new types of responses, and for typical preceding and subsequent sessions. Some analogous techniques in the training of pigeons, horses, and humans are discussed.},
author = {Pryor, Karen W. and Haag, Richard and O'Reilly, Joseph},
doi = {10.1901/jeab.1969.12-653},
issn = {0022-5002},
journal = {Journal of the Experimental Analysis of Behavior},
month = {jul},
number = {4},
pages = {653--661},
pmid = {16811388},
publisher = {Society for the Experimental Analysis of Behavior},
title = {{THE CREATIVE PORPOISE: TRAINING FOR NOVEL BEHAVIOR 1}},
url = {/pmc/articles/PMC1338662/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1338662/},
volume = {12},
year = {1969}
}
@misc{openai,
title = {{OpenAI Plays Hide and Seek{\ldots}and Breaks The Game! ? - YouTube}},
url = {https://www.youtube.com/watch?v=Lu56xVlZ40M{\&}t=63s},
urldate = {2021-08-27}
}
@article{KuczajIi2014,
abstract = {The variability of dolphin behavior is evident in their communication, foraging, and play. Dolphins can also vary their behavior when asked to do so by humans. Following the work of Karen Pryor and her colleagues, this ability is commonly referred to as creative behavior, a bit of a misnomer since dolphins need not always create a new behavior to succeed on this task. Nonetheless, when given a task in which success depends on not repeating what one has already done, dolphins are able to remember what they have done and successfully produce a new behavior, sometimes even a completely novel one. In this paper, we report similarities and differences in the performance of three bottlenose dolphins (Tursiops truncatus) given such a task, the results highlighting the need for continued investigation of individual differences in cognitive style and performance.},
author = {Kuczaj, Stan and Eskelinen, Holli},
doi = {10.12966/abc.02.05.2014},
file = {:Users/marcoyel21/Library/Application Support/Mendeley Desktop/Downloaded/Kuczaj Ii, Eskelinen - 2014 - The Creative Dolphin Revisited What Do Dolphins Do When Asked to Vary their Behavior.pdf:pdf},
issn = {23725052},
journal = {Animal Behavior and Cognition},
keywords = {Innovation,Tursiops truncatus,bottlenose dolphin,creativity,memory,novelty,variability},
number = {1},
pages = {66--76},
title = {{The “Creative Dolphin” Revisited: What Do Dolphins Do When Asked to Vary their Behavior?}},
volume = {1},
year = {2014}
}
@techreport{Audry,
author = {Audry, Sofian and Dumont-Gagn{\'{e}}, Rosalie and Scurto, Hugo},
file = {:Users/marcoyel21/Library/Application Support/Mendeley Desktop/Downloaded/Audry, Dumont-Gagn{\'{e}}, Scurto - Unknown - Behaviour Aesthetics of Reinforcement Learning in a Robotic Art Installation Behaviour Aesth(2).pdf:pdf},
title = {{Behaviour Aesthetics of Reinforcement Learning in a Robotic Art Installation Behaviour Aesthetics of Reinforcement Learning in a Robotic Art Installation. 4th NeurIPS Workshop on Machine Learning for Creativity and Design}},
url = {https://hal.archives-ouvertes.fr/hal-03100907}
}
@article{Baker2019,
abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
archivePrefix = {arXiv},
arxivId = {1909.07528},
author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
eprint = {1909.07528},
file = {:Users/marcoyel21/Library/Application Support/Mendeley Desktop/Downloaded/Baker et al. - 2019 - Emergent Tool Use From Multi-Agent Autocurricula.pdf:pdf},
month = {sep},
title = {{Emergent Tool Use From Multi-Agent Autocurricula}},
url = {https://arxiv.org/abs/1909.07528v2 http://arxiv.org/abs/1909.07528},
year = {2019}
}
